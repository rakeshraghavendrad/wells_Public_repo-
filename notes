
from pyspark.sql.functions import col, when

# Join the DataFrames on the 'id' column
df_combined = df1.alias("df1").join(df2.alias("df2"), on="id")

# List of columns to compare (excluding the join column)
columns_to_compare = ["col1", "col2"]

# Create a new DataFrame to show non-matching columns
non_matching_columns = df_combined.select("id", *[
    when(col(f"df1.{col}") != col(f"df2.{col}"), col(f"df1.{col}")).alias(f"{col}_non_matching")
    for col in columns_to_compare
])

# Filter out rows where all compared columns match
non_matching_columns = non_matching_columns.filter(
    " OR ".join([f"{col}_non_matching IS NOT NULL" for col in columns_to_compare])
)

non_matching_columns.show()Plotting
labels = ['Matching', 'Non-Matching']
counts = [matching_count, non_matching_count]

plt.figure(figsize=(8, 6))
plt.bar(labels, counts, color=['green', 'red'])
plt.xlabel('Comparison Result')
plt.ylabel('Count')
plt.title('Comparison of DataFrames')
plt.show()
