import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
import shap
import matplotlib.pyplot as plt

# Create a sample DataFrame
data = {
    'jarowinkler_similarity_firstname': [0.9, 0.7, 0.85, 0.65],
    'cosine_similarity_firstname': [95, 85, 90, 60],
    'jarowinkler_similarity_lastname': [0.8, 0.6, 0.75, 0.5],
    'cosine_similarity_lastname': [85, 70, 80, 50],
    'overall_score': [0.9, 0.7, 0.85, 0.65]  # Target/Label
}

df = pd.DataFrame(data)

# Features and target
X = df.drop('overall_score', axis=1)
y = df['overall_score']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the RandomForestRegressor
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Use KernelExplainer to calculate SHAP values
# Provide background data as a numpy array
explainer = shap.KernelExplainer(rf.predict, X_train.values)  # Use training data for background distribution
shap_values = explainer.shap_values(X_test.values)  # Also convert X_test to numpy array

# Display SHAP values for the first test instance
print("SHAP values for the first test instance:")
print(shap_values[0])  # SHAP values for the first test instance

# Create a summary plot
shap.summary_plot(shap_values, X_test)

# Create a force plot for the first instance
shap.initjs()
shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0])
