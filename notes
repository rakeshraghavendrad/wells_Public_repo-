from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Bucketing and Aggregating") \
    .getOrCreate()

# Assuming your DataFrame is named 'df'
# Bucketing predictions into 10 buckets
bucketed_df = df.withColumn("bucket", 
                    when((col("predictions") >= 0) & (col("predictions") < 10), "0-10") \
                    .when((col("predictions") >= 10) & (col("predictions") < 20), "10-20") \
                    .when((col("predictions") >= 20) & (col("predictions") < 30), "20-30") \
                    .when((col("predictions") >= 30) & (col("predictions") < 40), "30-40") \
                    .when((col("predictions") >= 40) & (col("predictions") < 50), "40-50") \
                    .when((col("predictions") >= 50) & (col("predictions") < 60), "50-60") \
                    .when((col("predictions") >= 60) & (col("predictions") < 70), "60-70") \
                    .when((col("predictions") >= 70) & (col("predictions") < 80), "70-80") \
                    .when((col("predictions") >= 80) & (col("predictions") < 90), "80-90") \
                    .when((col("predictions") >= 90) & (col("predictions") <= 100), "90-100") \
                    .otherwise("Out of range"))

# Grouping by bucket and calculating average response score
result_df = bucketed_df.groupBy("bucket").agg({"response_score": "avg"})

# Showing the result
result_df.show()







# Calculating mode for each bucket
mode_df = bucketed_df.groupBy("bucket").agg({"response_score": "count"}).withColumnRenamed("count(response_score)", "count")
mode_df = mode_df.join(bucketed_df, ["bucket"])
mode_df = mode_df.groupBy("bucket").agg({"response_score": "count"})
mode_df = mode_df.withColumnRenamed("count(response_score)", "max_count")
mode_df = mode_df.join(bucketed_df, ["bucket"])
mode_df = mode_df.groupBy("bucket", "response_score").agg({"response_score": "count"})
mode_df = mode_df.withColumnRenamed("count(response_score)", "count")
mode_df = mode_df.withColumn("rank", dense_rank().over(Window.partitionBy("bucket").orderBy(col("count").desc())))
mode_df = mode_df.filter(col("rank") == 1).drop("rank")
