https://arxiv.org/pdf/2107.00396

Dataset generation : no real images were used 
-> The entire dataset was synthetically generated using mock templates and artificial data.
-> The creators used publicly available document templates from sources like Wikimedia Commons.
-> Artificial faces were generated using tools like Generated Photos, which produce realistic-looking faces without using real people’s data.

so total 1000 dataset was created post this 
1.1 Scanning the Documents:
->Used high-resolution scanners to create sharp, clear images.
->Generated 2,000 scanned images (each document scanned twice with slight variations).
1.2 Photographing the Documents:
->Used various lighting conditions and angles to take photos.
->Ensured that documents were photographed both flat and at slight tilts.
->Captured 1,000 photographs to add diversity.
1.3 Video Recording:
->Recorded 1,000 video clips.
->Videos captured documents being moved, tilted, and rotated.
-> Each clip demonstrated dynamic perspectives.

Pertubations : 

2.1 Augmentation Techniques:
-> Lighting Variations: Introduced bright, dim, and uneven lighting conditions.
-> Perspective Distortion: Applied slight rotations and skews.
-> Motion Blur: Simulated shaky camera captures.
-> Background Variability: Placed documents against different backgrounds (tables, textured surfaces).

2.2 Resolution Adjustments:
-> Captured documents at various resolutions to mimic both high-quality and low-quality captures.
-> Some images were deliberately compressed to reflect mobile phone camera limitations.


https://arxiv.org/html/2408.01690v2#bib.bib2

1. Template Generation Using Image Diffusion Models from hugging face : 

-> Employed image diffusion models, such as Stable Diffusion, to remove existing texts and photos from sample ID images.
-> Masked customizable regions (e.g., name, photo areas) and prompted the model to inpaint these areas, generating clean templates.
-> For complex documents, iterative refinement with adjusted masks ensured high-quality template generation.

2. Fraud Pattern Simulation
-> To enhance the dataset's utility in fraud detection research, several fraudulent scenarios were synthetically generated:
-> Face Morphing: Blended two facial images to create a morphed portrait, simulating advanced identity fraud techniques.
-> Portrait Substitution: Replaced the original portrait with a disqualified or mismatched photo.
-> Text-Field Replacement: Altered specific textual fields (e.g., name, date of birth) with inconsistencies in font and formatting.
-> Mixed Fraud: Combined multiple fraud types within a single document.
-> Inpaint and Rewrite: Used inpainting techniques to modify specific fields without altering the overall layout.
-> Crop and Replace: Swapped sections between different IDs to simulate tampering.


https://arxiv.org/pdf/2409.12318

1. Participants and Data Collection:
The study involved 3,991 participants to assess the performance of Remote Identity Verification (RIdV) systems across diverse demographic groups.
Participants used their own devices (smartphones, tablets, laptops) in natural environments, introducing variability in camera quality, lighting conditions, and user proficiency.
This approach aimed to capture realistic user interactions rather than controlled lab conditions, ensuring the evaluation reflects practical use cases.

2. Demographic Information:
Collected data on age, gender, race/ethnicity, and skin tone.
Used the Monk Skin Tone Scale to categorize skin tones into 10 levels, allowing a nuanced analysis of skin tone-related performance differences.
Ensured inclusive representation to identify any biases related to age, gender, race, or skin tone in the RIdV system's performance.

1. System Evaluation:
1.1 Selecting Commercial RIdV Systems:
Objective:
To evaluate the effectiveness and fairness of five commercial Remote Identity Verification (RIdV) systems.

Criteria for Selection:
Chosen based on market presence and usage popularity.

Components Evaluated:
User Interface and Experience: How users interact with the system.
Image Capture Process: How documents and selfies are captured.
Document Verification: Accuracy of matching documents against database records.
Liveness Detection: Ability to detect fake or tampered photos.
Face Matching Algorithms: Accuracy in matching selfie images to the photo on the ID card.


2. Performance Metrics:
2.1 Key Metrics Evaluated:

False Negative Rate (FNR):
Measures how often genuine users are incorrectly rejected by the system.

Accuracy:
The overall correctness of identity verification.

Equity Analysis:
Comparison of FNR across demographic groups (age, gender, race, skin tone).

2.2 Statistical Analysis:
Significance Testing:
Used statistical methods to determine whether the differences in FNR between demographic groups were statistically significant.

Confidence Intervals:
Calculated to represent the reliability of the FNR estimates.

Error Bounds:
Considered to account for variability in data and sample size.






https://arxiv.org/pdf/2503.01085

Dataset used : 
 MIDV-500 Dataset
-> it offers real-world variability in:
-> Lighting conditions
-> Background settings
-> Document orientations
-> Device diversity (videos recorded using different mobile devices)

2.2 Preprocessing the Dataset:
Frame Extraction:
-> Extracted frames from each video clip at 10 frames per second (fps).
-> Focused on the first 3 seconds of each video to ensure consistency.
-> This resulted in approximately 15,000 image frames.
Image Annotation:
-> The MIDV-500 dataset already contains annotations specifying:
-> Document boundaries
-> Text fields
-> Photo areas
-> These annotations were utilized directly for training without additional modification.
-> The model demonstrated a high IoU (>0.8) and accuracy (>0.75) when detecting document boundaries.



https://pmc.ncbi.nlm.nih.gov/articles/PMC7297568/

1. Dataset Collection
Source: The dataset comprises 101 images of Colombian identity documents.
Consent: All documents were obtained with voluntary participant consent, ensuring ethical standards and data privacy.

2. Data Preprocessing
Image Cleaning: Applied techniques to remove noise and enhance image quality.
Normalization: Standardized image sizes and aspect ratios for consistency.
Color Conversion: Converted images to grayscale to reduce computational complexity.
Annotation: Key fields such as names, document numbers, and photos were annotated and stored in structured formats.

3. Feature Extraction
Text Detection: Utilized Optical Character Recognition (OCR) to extract textual information like ID numbers, names, and dates.
Image-Based Features: Employed techniques like Histogram of Oriented Gradients (HOG) and Scale-Invariant Feature Transform (SIFT) to extract distinctive image features.
Face Detection: Identified and extracted facial regions from the documents.

4. Model Training
Algorithms Used:

Convolutional Neural Networks (CNNs): For image-based verification, focusing on face matching and document layout recognition.
Support Vector Machines (SVMs): For classifying extracted textual features.
Random Forests and Decision Trees: For combining image and textual features.

Training Procedure:
Data Splitting: Divided into training (70%), validation (15%), and testing (15%) sets.
Hyperparameter Tuning: Adjusted parameters to optimize model performance.
Cross-Validation: Applied k-fold cross-validation to assess model robustness.


5. Model Evaluation
Performance Metrics:
Accuracy: Measured the correctness of the model's predictions.
Precision and Recall: Evaluated the model’s ability to detect authentic vs. forged IDs.
F1-Score: Balanced measure of precision and recall.
ROC-AUC Curve: Assessed the model’s discriminatory power.
