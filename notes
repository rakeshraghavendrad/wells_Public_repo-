from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Create a Spark session
spark = SparkSession.builder.appName("feature_classification").getOrCreate()

# Assuming your DataFrame is named 'df'
# Replace 'df' with your actual DataFrame name

# Define the conditions for segment classification
conditions = [
    (F.col("feature 1") + F.col("feature 2") + F.col("feature 3") + F.col("feature 4") +
     F.col("feature 5") + F.col("feature 6") >= 4, "seg4"),
    (F.col("feature 1") + F.col("feature 2") + F.col("feature 3") + F.col("feature 4") +
     F.col("feature 5") + F.col("feature 6") >= 2, "seg3"),
    (F.col("feature 1") + F.col("feature 2") + F.col("feature 3") + F.col("feature 4") +
     F.col("feature 5") + F.col("feature 6") >= 1, "seg2")
]

# Apply the conditions using 'when' and 'otherwise' functions
df_result = df.withColumn(
    "segment",
    F.when(F.coalesce("feature 1", "feature 2", "feature 3", "feature 4", "feature 5", "feature 6") >= 4, "seg4")
    .when(F.coalesce("feature 1", "feature 2", "feature 3", "feature 4", "feature 5", "feature 6") >= 2, "seg3")
    .when(F.coalesce("feature 1", "feature 2", "feature 3", "feature 4", "feature 5", "feature 6") >= 1, "seg2")
    .otherwise("unknown")
)

# Show the result
df_result.show()
