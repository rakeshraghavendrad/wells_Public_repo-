from pyspark.sql import functions as F

# Group by 'rate_bucket' and calculate count and average response score
avg_response_score = df_with_buckets.groupBy("rate_bucket") \
                                    .agg(F.count("cust_num").alias("count"),
                                         F.avg("rate").alias("avg_response_score"))

# Show the distribution of 'cust_num' and average response score in each bucket
avg_response_score.show()
