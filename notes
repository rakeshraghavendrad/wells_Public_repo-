I am currently running a Spark cluster on my pod, and I have noticed that execution times have significantly increased. After running for a while, the process fails due to memory issues.

Previously, I was able to successfully label 50,000 records without encountering such problems. However, over the past 2-3 days, this issue has become consistent. I suspect that the memory allocated to my pod might have been reduced compared to earlier settings.

Could you kindly investigate and address this on priority?
