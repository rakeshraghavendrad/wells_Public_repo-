from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Create a Spark session
spark = SparkSession.builder.appName("CustomerSegmentTracking").getOrCreate()

# Your dataset
data = [
    (12345, 202301, 0, 0, 0, 0, 0, 0, 1),
    # ... (add all your data rows here)
]

# Define the schema
schema = ["cust_num", "date", "feature1", "feature2", "feature3", "feature4", "feature5", "feature6", "segment"]

# Create a DataFrame
df = spark.createDataFrame(data, schema=schema)

# Define a window specification to order by date
window_spec = Window.partitionBy("cust_num").orderBy("date")

# Create a new column to track the change in segment
df = df.withColumn("segment_change", F.lag("segment").over(window_spec))

# Filter the DataFrame to include only the rows where the segment changes from 1 to 4
movement_df = df.filter((df.segment_change == 1) & (df.segment == 4))

# Show the result
movement_df.select("cust_num", "date", "segment_change", "segment").show()

# Stop the Spark session
spark.stop()
