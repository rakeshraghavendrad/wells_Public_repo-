from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("ExampleApp").getOrCreate()

# Example DataFrames
data1 = [(1, 'A'), (2, 'B'), (3, 'C')]
data2 = [(1, 'A'), (4, 'D'), (5, 'E')]

columns = ['col1', 'value']

sd1 = spark.createDataFrame(data1, columns)
sd2 = spark.createDataFrame(data2, columns)

# Perform the inner join
df_inner = sd1.join(sd2, sd1.col1 == sd2.col1, 'inner')

# Get the rows not part of the inner join from sd1
df_out_sd1 = sd1.subtract(df_inner.select(sd1.columns))

# Get the rows not part of the inner join from sd2
df_out_sd2 = sd2.subtract(df_inner.select(sd2.columns))

# Combine the rows not part of the inner join from both DataFrames
df_out = df_out_sd1.union(df_out_sd2)

df_out.show()
