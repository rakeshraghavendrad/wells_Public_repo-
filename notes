
from pyspark.sql import SparkSession
from pyspark.sql.types import StringType, StructType, StructField

# Initialize Spark session
spark = SparkSession.builder.master("local").appName("JoinExample").getOrCreate()

# Create sample data for DF1 and DF2
data1 = [("A", "Data1_A1", "Data1_A2"), ("B", "Data1_B1", "Data1_B2"), ("C", "Data1_C1", "Data1_C2")]
data2 = [("A", "Data2_A1", "Data2_A2"), (None, "Data2_Null", "Data2_Null"), ("B", "Data2_B1", "Data2_B2")]

# Define schema
schema1 = StructType([StructField("col1", StringType(), True), 
                      StructField("col2", StringType(), True), 
                      StructField("col3", StringType(), True)])

schema2 = StructType([StructField("col1", StringType(), True), 
                      StructField("col4", StringType(), True), 
                      StructField("col5", StringType(), True)])

# Create DataFrames
df1 = spark.createDataFrame(data1, schema1)
df2 = spark.createDataFrame(data2, schema2)

# Perform inner join
joined_df = df1.join(df2, df1.col1 == df2.col1, "inner")

# Show the result
joined_df.show()
