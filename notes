from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Create a Spark session
spark = SparkSession.builder.appName("segment_classification").getOrCreate()

# Sample data
data = [
    (12345, None, 202301, 0, 0, 0, 0, 0, 0, 1),
    (None, 12345, 202302, 4, 0, 0, 0, 0, 0, 2),
    (None, 12345, 202303, 0, 0, 0, 0, 0, 1, 2),
    (None, 12345, 202304, 0, 0, 3, 1, 0, 0, 3),
    (None, 12345, 202305, 0, 1, 0, 0, 0, 1, 3),
    (None, 12345, 202306, 0, 0, 1, 1, 1, 0, 3),
    (None, 12345, 202307, 1, 1, 7, 1, 0, 0, 4),
    (None, 12345, 202308, 1, 1, 2, 1, 1, 0, 4)
]

# Define schema
schema = ["cust_num_seg", "cust_num", "date", "feature1", "feature2", "feature3", "feature4", "feature5", "feature6", "segment"]

# Create a DataFrame
df = spark.createDataFrame(data, schema=schema)

# Define a window specification based on cust_num
window_spec = Window.partitionBy("cust_num").orderBy("date").rangeBetween(Window.unboundedPreceding, Window.currentRow)

# Count the number of features used for each cust_num
feature_columns = ["feature1", "feature2", "feature3", "feature4", "feature5", "feature6"]
for col in feature_columns:
    df = df.withColumn(col + "_count", F.sum(col).over(window_spec))

# Determine the segment based on the feature counts
df = df.withColumn(
    "segment",
    F.when(df["feature1_count"] >= 1, "seg2")
    .when(df["feature1_count"] + df["feature2_count"] >= 2, "seg3")
    .when(F.array([df[col + "_count"] for col in feature_columns]).sum() >= 4, "seg4")
    .otherwise(None)
)

# Drop the temporary count columns
df = df.drop(*[col + "_count" for col in feature_columns])

# Show the result
df.show(truncate=False)
