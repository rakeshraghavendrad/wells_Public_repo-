from pyspark.sql import SparkSession

# Initialize Spark session
spark = SparkSession.builder.appName("CheckColumns").getOrCreate()

# Assuming DF is already defined, for example:
# DF = spark.read.csv("path_to_your_file.csv", header=True, inferSchema=True)

# List of columns to check
columns_to_check = ['c1', 'c2', 'c3']

# Get the list of columns in the DataFrame
df_columns = DF.columns

# Define functions for each column
def c1_col():
    print('Function for missing column c1 executed')

def c2_col():
    print('Function for missing column c2 executed')

def c3_col():
    print('Function for missing column c3 executed')

# Map column names to functions
functions_map = {
    'c1': c1_col,
    'c2': c2_col,
    'c3': c3_col
}

# Check if all required columns exist in the DataFrame
columns_exist = all(column in df_columns for column in columns_to_check)

# Print the result and execute missing column functions if any
if columns_exist:
    print("All columns exist in the DataFrame")
else:
    missing_columns = [column for column in columns_to_check if column not in df_columns]
    print(f"The following columns are missing: {missing_columns}")
    
    # Execute the function for each missing column
    for column in missing_columns:
        functions_map[column]()

# Return the DataFrame
DF
