from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when

# Create Spark session
spark = SparkSession.builder.appName("ColumnComparison").getOrCreate()

# Sample data for DF1
data1 = [(1, 'A', 101), (2, 'B', 102), (3, 'C', 103), (4, 'D', 104)]
columns1 = ["C1", "C2", "C3"]
DF1 = spark.createDataFrame(data1, columns1)

# Sample data for DF2
data2 = [(1, 'X', 101), (2, 'Y', 102), (3, 'C', 103), (5, 'Z', 105)]
columns2 = ["D1", "D2", "D3"]
DF2 = spark.createDataFrame(data2, columns2)

# Combine both dataframes based on row index
DF1 = DF1.withColumn("row_idx", F.monotonically_increasing_id())
DF2 = DF2.withColumn("row_idx", F.monotonically_increasing_id())

combined_df = DF1.join(DF2, "row_idx", "inner").drop("row_idx")

# List of column pairs to compare
column_pairs = [("C1", "D1"), ("C2", "D2"), ("C3", "D3")]

# Loop through the column pairs and add comparison columns
for col1, col2 in column_pairs:
    comparison_col_name = f"{col1}_{col2}_is_match"
    combined_df = combined_df.withColumn(comparison_col_name, when(col(col1) == col(col2), True).otherwise(False))

# Show the result
combined_df.show()
