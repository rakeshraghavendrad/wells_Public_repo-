import pyspark.sql.functions as f

# Filter the DataFrame to get rows between specific dates
df_filtered = df.filter((f.col('column1') >= '2024-09-01') & (f.col('column1') <= '2024-09-30'))

# Extract the date from the datetime column (assuming 'column1' is a timestamp)
df_with_date = df_filtered.withColumn('date', f.to_date(f.col('column1')))

# Group by the 'date' column and count the occurrences of 'column5'
df_daily_count = df_with_date.groupBy('date').agg(f.count('column5').alias('count_column5'))

# Show the result
df_daily_count.show()
