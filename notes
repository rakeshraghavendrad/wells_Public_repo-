from pyspark.sql import SparkSession
from pyspark.sql import functions as F

# Create a Spark session
spark = SparkSession.builder.appName("feature_classification").getOrCreate()

# Assuming your DataFrame is named 'df'
# Replace 'df' with your actual DataFrame name

# Define the conditions for segment classification
conditions = [
    (F.col("feature 1") + F.col("feature 2") + F.col("feature 3") + F.col("feature 4") +
     F.col("feature 5") + F.col("feature 6") >= 4, "seg4"),
    (F.col("feature 1") + F.col("feature 2") + F.col("feature 3") + F.col("feature 4") +
     F.col("feature 5") + F.col("feature 6") >= 2, "seg3"),
    (F.col("feature 1") + F.col("feature 2") + F.col("feature 3") + F.col("feature 4") +
     F.col("feature 5") + F.col("feature 6") >= 1, "seg2")
]

# Additional condition for "idle" classification
idle_condition = (
    (F.col("feature 1") == 0) & (F.col("feature 2") == 0) & (F.col("feature 3") == 0) &
    (F.col("feature 4") == 0) & (F.col("feature 5") == 0) & (F.col("feature 6") == 0)
)

# Apply the conditions using 'when' and 'otherwise' functions
df_result = df.withColumn(
    "segment",
    F.when((F.col("feature 1") >= 1) | (F.col("feature 2") >= 1) | (F.col("feature 3") >= 1) |
           (F.col("feature 4") >= 1) | (F.col("feature 5") >= 1) | (F.col("feature 6") >= 1), "seg2")
    .when((F.col("feature 1") >= 2) | (F.col("feature 2") >= 2) | (F.col("feature 3") >= 2) |
          (F.col("feature 4") >= 2) | (F.col("feature 5") >= 2) | (F.col("feature 6") >= 2), "seg3")
    .when((F.col("feature 1") >= 4) | (F.col("feature 2") >= 4) | (F.col("feature 3") >= 4) |
          (F.col("feature 4") >= 4) | (F.col("feature 5") >= 4) | (F.col("feature 6") >= 4), "seg4")
    .when(idle_condition, "idle")
    .otherwise("unknown")
)

# Show the result
df_result.show()
