Cheos theory --> 

def identify_address_patterns(df):
    """
    Identifies hidden patterns in address similarity scores by analyzing 
    relationships between Levenshtein, Jaro-Winkler, and Fuzzy Partial Ratio.

    Args:
        df (pd.DataFrame): DataFrame containing similarity scores.

    Returns:
        pd.DataFrame: DataFrame with a new 'Pattern' column indicating the identified pattern.
    """
    patterns = []
    
    for _, row in df.iterrows():
        lev, jaro, fuzzy = row['Levenshtein'], row['Jaro-Winkler'], row['Fuzzy Partial Ratio']

        # All scores are similar → Minor Formatting Issue (Spacing, Case, Abbreviations)
        if abs(lev - jaro) < 0.02 and abs(jaro - fuzzy) < 0.02:
            patterns.append("Minor Formatting Issue")

        # Levenshtein drops significantly → Likely a Typo or Character-Level Change
        elif lev < jaro and lev < fuzzy:
            patterns.append("Typo / Spelling Error")

        # Jaro-Winkler drops significantly → Likely Reordering of Words
        elif jaro < lev and jaro < fuzzy:
            patterns.append("Reordering Detected")

        # Fuzzy Partial Ratio drops significantly → Missing/Extra Words
        elif fuzzy < lev and fuzzy < jaro:
            patterns.append("Missing/Extra Words")

        # Unclear pattern → Needs further analysis
        else:
            patterns.append("Unclassified / Needs Review")

    df['Pattern'] = patterns
    return df
-----------------------------------------------------------------------------------------------------------------------


import pandas as pd

def classify_address_similarity(row):
    """
    Identifies patterns in address similarity scores and classifies them into groups 
    without using clustering algorithms.
    """
    lev = row['Levenshtein']
    jaro = row['Jaro-Winkler']
    fuzzy = row['Fuzzy Partial Ratio']

    # Find the lowest score
    min_score = min(lev, jaro, fuzzy)
    
    # Rule-based grouping based on similarity behavior
    if lev == min_score and jaro > lev and fuzzy > lev:
        return "Character-level differences (Typo / Spelling Error)"
    elif jaro == min_score and lev > jaro and fuzzy > jaro:
        return "Ordering mismatch (Reordering Issue)"
    elif fuzzy == min_score and lev > fuzzy and jaro > fuzzy:
        return "Structural mismatch (Missing/Extra Words)"
    elif abs(lev - jaro) < 0.03 and abs(jaro - fuzzy) < 0.03 and abs(lev - fuzzy) < 0.03:
        return "Minor Formatting Differences (Spaces, Abbreviations)"
    else:
        return "Unclassified Pattern (Needs Review)"

# Sample data (Levenshtein, Jaro-Winkler, Fuzzy Partial Ratio)
df = pd.DataFrame([
    [1.0, 1.0, 1.0],  # Identical addresses
    [0.95, 0.94, 0.95],  # Minor formatting issue (space missing)
    [0.91, 0.93, 0.88],  # Missing/extra word
    [0.94, 0.85, 0.95],  # Reordering
    [0.87, 0.93, 0.94],  # Typo / spelling error
    [0.92, 0.92, 0.92],  # Balanced similarity (uncertain classification)
    [0.85, 0.85, 0.92],  # Strong typo/misspelling
    [0.90, 0.90, 0.90],  # Close similarity but unclear classification
], columns=['Levenshtein', 'Jaro-Winkler', 'Fuzzy Partial Ratio'])

# Apply pattern classification
df['Pattern Classification'] = df.apply(classify_address_similarity, axis=1)

# Display the categorized results
import ace_tools as tools
tools.display_dataframe_to_user(name="Address Similarity Pattern Analysis", dataframe=df)

-----------------------------------------------------------------------------------------------------------------------------

import numpy as np
import difflib

def address_to_numeric(address):
    """
    Converts an address string into a numerical sequence based on ASCII values.
    """
    return np.array([ord(char) for char in address])

def compute_lyapunov_exponent(address1, address2):
    """
    Computes the Lyapunov Exponent based on character-level divergence between two addresses.
    """
    # Convert addresses into numerical sequences
    seq1 = address_to_numeric(address1)
    seq2 = address_to_numeric(address2)

    # Ensure sequences are of the same length by padding the shorter one
    max_length = max(len(seq1), len(seq2))
    seq1 = np.pad(seq1, (0, max_length - len(seq1)), mode='constant', constant_values=0)
    seq2 = np.pad(seq2, (0, max_length - len(seq2)), mode='constant', constant_values=0)

    # Compute the differences at each step
    diffs = np.abs(seq1 - seq2)

    # Compute the Lyapunov Exponent (logarithmic divergence measurement)
    norm_diffs = np.linalg.norm(diffs)
    return np.log(np.abs(norm_diffs + 1e-10))  # Avoid log(0) issues

# Sample Address Pairs
addresses = [
    ("123 Main Street", "123 Main Street"),  # Identical
    ("123 Main Street", "123 Main St"),  # Abbreviation
    ("123 Main Street", "Main Street 123"),  # Reordering
    ("123 Main Street", "St Main 123"),  # More complex reordering
    ("123 Main Street", "123 Maine Street"),  # Typo in street name
    ("123 Main Street", "456 Elm Road"),  # Completely different address
]

# Compute Lyapunov Exponent for each address pair
results = []
for addr1, addr2 in addresses:
    lyap_exp = compute_lyapunov_exponent(addr1, addr2)
    results.append((addr1, addr2, lyap_exp))

# Convert results to DataFrame for display
df_results = pd.DataFrame(results, columns=["Address 1", "Address 2", "Lyapunov Exponent"])

# Categorize Lyapunov Exponent into stability classes
def categorize_lyapunov(lyap_exp):
    if lyap_exp < -2.0:
        return "Stable Address (Minimal Divergence)"
    elif -2.0 <= lyap_exp < -1.0:
        return "Minor Variations (Predictable Change)"
    elif -1.0 <= lyap_exp < 0:
        return "Moderate Chaos (Typos, Small Changes)"
    elif 0 <= lyap_exp < 1.0:
        return "Chaotic Address (Reordering or Missing Words)"
    else:
        return "Extreme Chaos (Completely Different Address)"

df_results["Chaos Classification"] = df_results["Lyapunov Exponent"].apply(categorize_lyapunov)

# Display results
import ace_tools as tools
tools.display_dataframe_to_user(name="Lyapunov Exponent for Address Similarity", dataframe=df_results)



